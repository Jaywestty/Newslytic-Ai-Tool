{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaywestty/News-Crime-Classification/blob/main/News_Text_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAS3oFqUUf_F"
      },
      "source": [
        "### **NEWS TEXT SUMMARIZER PROJECT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TshE3YFxJ0rP"
      },
      "source": [
        "####**Project Description:**\n",
        "This project aims to automatically summarize news articles into concise, factual highlights using Hugging Face Transformers. The summarization model is based on the bart-base architecture, chosen for its strong performance on abstractive summarization while remaining lightweight enough to run within Google Colab's free-tier resource limits. The dataset, sourced from Hugging Face’s public datasets repository, contains diverse news articles for training and evaluation. The system is designed to generate short, accurate, and easily readable summaries that retain the key points of the original article, making it useful for quick news consumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUYkV0fBWQfm"
      },
      "source": [
        "#### **Install dependecies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_Dv9hJc9AIn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate rouge_score accelerate nltk -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erZVTiIKKr3o"
      },
      "source": [
        "#### **Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBh79AK2VPDY"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import gc\n",
        "import evaluate\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNWZ2DgO2oow"
      },
      "outputs": [],
      "source": [
        "#Clear up memory to aid colab limit\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWXJ608OK4IS"
      },
      "source": [
        "#### **Load BART Tokenizer and Model**  \n",
        "\n",
        "For fine-tuning, we load the **BART-base** model and tokenizer directly from Hugging Face.  \n",
        "\n",
        "- **BART-base** is chosen over **T5-small** because:  \n",
        "  - It generally produces **higher-quality summaries**.  \n",
        "  - It balances performance with efficiency, making it suitable for **Colab free tier GPUs**.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAFicTXP23sx"
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRDuY5nTLFQF"
      },
      "source": [
        "#### **Load Dataset**\n",
        "\n",
        "We use the **CNN/DailyMail dataset** provided by Hugging Face Datasets.  \n",
        "\n",
        "Due to restricted GPU access on Colab, we work with a **subset**:  \n",
        "- 8,000 samples from the **training set**  \n",
        "- 800 samples from the **validation set**  \n",
        "- 800 samples from the **test set**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZZGU0BQ3fek"
      },
      "outputs": [],
      "source": [
        "print(\"Loading CNN-DailyMail dataset...\")\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "\n",
        "print(\"Sample article:\\n\", dataset['train'][0]['article'][:200])\n",
        "print(\"\\nSample summary:\\n\", dataset['train'][0]['highlights'])\n",
        "\n",
        "# Reduce dataset for Colab constraints\n",
        "train_dataset = dataset['train'].select(range(8000))  # Slightly smaller for BART\n",
        "val_dataset = dataset['validation'].select(range(800))\n",
        "test_dataset = dataset['test'].select(range(800))\n",
        "\n",
        "print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEOQagRqLSJw"
      },
      "source": [
        "#### **BART-specific preprocessing**\n",
        "\n",
        "## Data Preprocessing  \n",
        "Before training, we need to prepare the text for BART:  \n",
        "- Tokenize the input and target texts  \n",
        "- Truncate or pad sequences to a fixed length  \n",
        "- Format inputs and labels for Seq2Seq training  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUeOer1YBJhl"
      },
      "outputs": [],
      "source": [
        "max_input_length = 1024  # BART can handle longer inputs\n",
        "max_target_length = 142  # CNN-DM standard summary length\n",
        "\n",
        "def preprocess(example):\n",
        "    model_inputs = tokenizer(\n",
        "        example['article'],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"  # Changed from True\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            example['highlights'],\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Replace pad token id with -100 for label loss masking\n",
        "    labels_ids = labels[\"input_ids\"]\n",
        "    labels_ids = [\n",
        "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
        "        for label in labels_ids\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels_ids\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "print(\"Preprocessing datasets...\")\n",
        "train_tokenized = train_dataset.map(preprocess, batched=True, remove_columns=train_dataset.column_names)\n",
        "val_tokenized = val_dataset.map(preprocess, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vJYmqxxQ9Ud"
      },
      "outputs": [],
      "source": [
        "print(train_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiQnskkmjJSN"
      },
      "outputs": [],
      "source": [
        "print(\"Pad token ID:\", tokenizer.pad_token_id)\n",
        "print(\"Vocab size:\", tokenizer.vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z1BRQQ8Nzfk"
      },
      "source": [
        "#### **Load ROUGE for Evaluation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVTPT9Lh_beX"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = [[(token if token != -100 else tokenizer.pad_token_id) for token in label] for label in labels]\n",
        "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
        "    return {k: round(v * 100, 2) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHZd5CIBN6HB"
      },
      "source": [
        "#### **Train the Model**\n",
        "\n",
        "The model is trained on the reduced dataset:  \n",
        "- Training loss is logged  \n",
        "- Validation loss is tracked for overfitting  \n",
        "- Best checkpoint is saved  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9yAk4dW_qp5"
      },
      "outputs": [],
      "source": [
        "#Training Arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./bart-news-summarizer\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,   # Increase to 3 (sweet spot for Colab free tier)\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,   # Log progress every 100 steps\n",
        "    save_strategy=\"epoch\"  # Save at the end of each epoch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RCQM2CB_5Qx"
      },
      "outputs": [],
      "source": [
        "#Trainer Setup\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear_memory()\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "J2To-j4y4q5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Best Model Selection**  \n",
        "\n",
        "After training, **epoch 2** was identified as the best-performing checkpoint.  \n",
        "\n",
        "- **Training Loss:** 1.60  \n",
        "- **Validation Loss:** 2.23  \n",
        "- **ROUGE-1:** 24.6  \n",
        "- **ROUGE-2:** 9.5  \n",
        "- **ROUGE-Lsum:** 22.48  \n",
        "\n",
        "These results show that by the second epoch, the model had already reached an optimal balance between **low loss** and **high ROUGE scores**, indicating strong summarization quality without overfitting.  \n",
        "\n",
        "For this reason, **epoch 2 is saved as the final model checkpoint**, which will be used for deployment.  \n"
      ],
      "metadata": {
        "id": "oPDPromgl0DF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BlZ7H75OWiU"
      },
      "source": [
        "#### **Evaluation of Model**\n",
        "\n",
        "We evaluate the model using:  \n",
        "- **ROUGE scores** (ROUGE-1, ROUGE-2, ROUGE-L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsjT_YpGR89H"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7l3Y1dOdtq"
      },
      "source": [
        "#### **Testing Best Checkpoint (Epoch 2)  on an article**\n",
        "\n",
        "Before saving, we reload the **epoch 2 checkpoint** to verify its performance on a sample article.  \n",
        "This step ensures that the model generates **coherent and concise summaries** before committing it as our final saved version.  \n",
        "\n",
        "By testing on real input text, we can confirm that the chosen checkpoint generalizes well beyond the validation set.  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Point directly to epoch 2 checkpoint\n",
        "model_path = \"./bart-news-summarizer/checkpoint-8000\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "D-A3Cu_GDwqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "tM-mTRwUED4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1WbJxBlBBUM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_and_merge_article(article):\n",
        "    # Step 1: Clean article text\n",
        "    article = re.sub(r\"\\s+\", \" \", article.strip())  # collapse spaces & newlines\n",
        "    article = article.replace(\" ,\", \",\").replace(\" .\", \".\")  # fix space before punctuation\n",
        "\n",
        "    # Step 2: Summarize using your model\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True).to(model.device)\n",
        "    summary_ids = model.generate(**inputs, max_length=200, min_length=80, length_penalty=2.0, num_beams=4)\n",
        "    raw_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Step 3: Merge summary into one sentence\n",
        "    summary = re.sub(r'\\s+', ' ', raw_summary.strip())\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', summary)\n",
        "    sentences = [s.strip(\" .\") for s in sentences if s.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        return \"\"\n",
        "    if len(sentences) == 1:\n",
        "        return sentences[0] + \".\"\n",
        "\n",
        "    merged = \", \".join(sentences[:-1]) + \" and \" + sentences[-1]\n",
        "    return merged.strip() + \".\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZO5O1EUFfYg"
      },
      "outputs": [],
      "source": [
        "article = \"\"\"\n",
        "The Vice Chancellor, Federal University of Technology and Environmental Sciences, Iyin Ekiti, Prof. Gbenga Aribisala, has said that the new institution will begin the admission process in September.\n",
        "\n",
        "Aribisala said that the admission process would follow the National Universities Commission Resource Verification exercise taking place soon in the university.\n",
        "\n",
        "The VC, who spoke in Ado Ekiti on Sunday at a reception to celebrate the 90th birthday of his mother, Deaconess Felicia Aribisala, also canvassed support from well-meaning Nigerians to the institution, saying, “A technology-based institution of this nature is capital-intensive”.\n",
        "\n",
        "He said, “The NUC is coming for Resource Verification of all the 36 programmes that we are trying to offer. As soon as they come, by the special grace of God, we have provided those things that will be needed.\n",
        "“We have provided a modern laboratory for all the programmes. We have a library now. We have classrooms fixed.\n",
        "\n",
        "“We have offices and furniture fixed. We have all of those things. So we are very confident we are going to scale through.\n",
        "\n",
        "“By the time we now scale through, by the special grace of God, by September this year, we are going to ask those who are interested in our university to do Change of University, and admission will begin. That is the icing. And after that, recruitment of staff will just follow”.\n",
        "\n",
        "The VC, who said that funding of education should not be left to the government alone, said, “Universities need a lot of funding. Funding is a major challenge. You have to provide facilities and all of those things.\n",
        "“So, as I speak to you, we (FUTES) do not have enough funds. That’s why we keep appealing and going to people because the government cannot do it all alone. We have been visiting some people who are public-spirited, people who like education, tertiary education.\n",
        "\n",
        "“If we have people who want to donate buildings, we are going to name such after them; people who want to give scholarships; people who want to build hostels in such a manner that it is their own and they will take rent and all of those things.\n",
        "\n",
        "“I think the funding is crucial because if you look at the nature of our university, University of Technology and Environmental Sciences, it is capital-intensive, it is technology-based.\n",
        "\n",
        "“It means we need a lot of equipment. As I said, the government cannot do everything. So we need help at this time financially,” the VC said.\n",
        "\n",
        "Aribisala disclosed that the land issue, which could have been a challenge to the university, had been resolved amicably with an agreement made with the concerned families.\n",
        "\n",
        "“As I speak to you now, it has been resolved. The 200 hectares that have been donated to the university are very intact.\n",
        "\n",
        "“There has been an agreement. The community and government will also pay some compensation to the families.\n",
        "\n",
        "“So they are now at peace. The community is not trying to force the land. I think that was the kind of misconception that happened at the time,” the Vice Chancellor said.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm9nv0tLF5p5"
      },
      "outputs": [],
      "source": [
        "clean = clean_and_merge_article(article)\n",
        "print(clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHGPiN9sVSa"
      },
      "source": [
        "### **Model Saving and Deployment **\n",
        "\n",
        "With **epoch 2** identified as the best-performing checkpoint, we save this model for future use.  \n",
        "\n",
        "The saved model can be:  \n",
        "- **Reloaded locally** for inference or further fine-tuning  \n",
        "- **Uploaded to Hugging Face Hub** to make it publicly accessible  \n",
        "- **Integrated into applications** (e.g., Flask, FastAPI, or Streamlit apps) for real-world summarization tasks  \n",
        "\n",
        "This ensures that our best model is preserved and can be easily deployed for production-level usage.  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "NejNqBFTN5K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "checkpoint_path = \"./bart-news-summarizer/checkpoint-8000\"  # <-- your epoch2 path\n",
        "save_path = \"./bart-summarizer-epoch2\"  # final folder you’ll save\n",
        "\n",
        "# load checkpoint\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "# save clean copy\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Epoch2 model saved at {save_path}\")"
      ],
      "metadata": {
        "id": "9NUQjG6mO8gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(save_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "\n",
        "print(\"✅ Reloaded epoch2 model successfully\")"
      ],
      "metadata": {
        "id": "5YIxJsAiQMJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXy0mHYguhhD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "repo_id = \"Jayywestty/bart-summarizer-epoch2\"  # change to your HF username/repo\n",
        "\n",
        "# load from your already cleaned folder\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./bart-summarizer-epoch2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./bart-summarizer-epoch2\")\n",
        "\n",
        "# push to hub\n",
        "model.push_to_hub(repo_id)\n",
        "tokenizer.push_to_hub(repo_id)\n",
        "\n",
        "print(f\"✅ Model uploaded to https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKW3rSsnR5Yy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM/bWFNk+dEfWIcsiplr+MD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}